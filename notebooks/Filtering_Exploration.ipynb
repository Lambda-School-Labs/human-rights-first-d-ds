{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Filtering Time Exploration.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Hz4dteDHTHey"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz4dteDHTHey",
        "colab_type": "text"
      },
      "source": [
        "## Set up imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBpu2wzqlMeJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "de7abbfc-9752-478a-bde0-6341ac4638f2"
      },
      "source": [
        "# install PRAW and newspaper3k\n",
        "!pip install praw\n",
        "!pip3 install newspaper3k"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting praw\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/15/4bcc44271afce0316c73cd2ed35f951f1363a07d4d5d5440ae5eb2baad78/praw-7.1.0-py3-none-any.whl (152kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 1.7MB/s \n",
            "\u001b[?25hCollecting websocket-client>=0.54.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 5.1MB/s \n",
            "\u001b[?25hCollecting update-checker>=0.17\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/ba/8dd7fa5f0b1c6a8ac62f8f57f7e794160c1f86f31c6d0fb00f582372a3e4/update_checker-0.18.0-py3-none-any.whl\n",
            "Collecting prawcore<2.0,>=1.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/40/b741437ce4c7b64f928513817b29c0a615efb66ab5e5e01f66fe92d2d95b/prawcore-1.5.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.54.0->praw) (1.15.0)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from update-checker>=0.17->praw) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.17->praw) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.17->praw) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.17->praw) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.17->praw) (3.0.4)\n",
            "Installing collected packages: websocket-client, update-checker, prawcore, praw\n",
            "Successfully installed praw-7.1.0 prawcore-1.5.0 update-checker-0.18.0 websocket-client-0.57.0\n",
            "Collecting newspaper3k\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 1.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (7.0.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.2.5)\n",
            "Collecting tldextract>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/cf/d0ff82625e53bd245d6173ce6333d190abbfcd94e4c30e54b4e16b474216/tldextract-2.2.3-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.6.3)\n",
            "Collecting tinysegmenter==0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.2.6)\n",
            "Collecting feedparser>=5.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/84/df6de99cba01afc82344c9cb3a79df100a00ac33396120f8aa66c72f0d84/feedparser-6.0.1-py2.py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.9MB/s \n",
            "\u001b[?25hCollecting jieba3k>=0.35.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 5.1MB/s \n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.13)\n",
            "Collecting cssselect>=0.9.2\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.2.1->newspaper3k) (1.15.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.10)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
            "Collecting sgmllib3k\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2020.6.20)\n",
            "Building wheels for collected packages: tinysegmenter, jieba3k, feedfinder2, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp36-none-any.whl size=13538 sha256=d91f37074127e940e634f94e067574c092fe4d007b6f78703619f11f196b4dcc\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp36-none-any.whl size=7398406 sha256=67795cd80b156fed119ae636ad44420be06e8d14a80ceaf2f0b2b312495c8911\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp36-none-any.whl size=3355 sha256=6b93d8dbb099591d515032ba48ba27639d489d6ea430d1559121fa1664f83edd\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp36-none-any.whl size=6067 sha256=4a5ac7792626b3bd79f128b5c4df3b863f6e23160bbbd9fd1b9214fab7da5e7b\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
            "Successfully built tinysegmenter jieba3k feedfinder2 sgmllib3k\n",
            "Installing collected packages: requests-file, tldextract, tinysegmenter, sgmllib3k, feedparser, jieba3k, feedfinder2, cssselect, newspaper3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.1 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-2.2.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5a8sCRHTJl9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import praw\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pickle\n",
        "from newspaper import Article\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from datetime import datetime"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnGx9Ot62LeJ",
        "colab_type": "text"
      },
      "source": [
        "## Set up various objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk-B5VF8RGNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lowerify(text):\n",
        "    # fix up geolocation dataframe a little\n",
        "    return text.lower()\n",
        "\n",
        "# set up cities/states locations datafrane\n",
        "locs_path = 'https://raw.githubusercontent.com/Lambda-School-Labs/Labs25-Human_Rights_First-TeamB-DS/main/project/cities_states.csv'\n",
        "locs_df = pd.read_csv(locs_path)\n",
        "locs_df = locs_df.drop(columns=['Unnamed: 0', 'country'])\n",
        "locs_df['city_ascii'] = locs_df['city_ascii'].apply(lowerify)\n",
        "locs_df['admin_name'] = locs_df['admin_name'].apply(lowerify)\n",
        "\n",
        "# state to city lookup table\n",
        "states_map = {}\n",
        "for state in list(locs_df.admin_name.unique()):\n",
        "    states_map[state] = locs_df[locs_df['admin_name'] == state]['city_ascii'].to_list()\n",
        "\n",
        "# police brutality indentifying nlp\n",
        "# make sure to import model.pkl\n",
        "model_file = open('model.pkl', 'rb')\n",
        "pipeline = pickle.load(model_file)\n",
        "model_file.close()\n",
        "\n",
        "# spacy nlp model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Set up PRAW\n",
        "# PRAW credentials go here"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOWGb6c8xxgr",
        "colab_type": "text"
      },
      "source": [
        "## Run the update and see what's returned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pETEmO8z2DRO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "db6cbc2b-0fcd-493b-b468-9bb334f56ed8"
      },
      "source": [
        "# Grab data from reddit\n",
        "data = []\n",
        "print('Pulling data from Reddit...')\n",
        "for submission in reddit.subreddit(\"news\").top('week', limit=500):\n",
        "    data.append([\n",
        "        submission.id, submission.title, submission.url\n",
        "    ])\n",
        "# construct a dataframe with the data\n",
        "col_names = ['id', 'title', 'url']\n",
        "df = pd.DataFrame(data, columns=col_names)\n",
        "print(f'Number of entries initially pulled: {df.shape[0]}\\n')\n",
        "\n",
        "# pull the text from each article itself using newspaper3k\n",
        "content_list = []\n",
        "date_list = []\n",
        "# go through each URL and use newspaper3k to extract data\n",
        "print('Extracting data via newspaper3k...')\n",
        "for id_url in df['url']:\n",
        "    # use newspaper3k to extract text\n",
        "    article = Article(id_url)\n",
        "    article.download()\n",
        "    # if the article doesn't download, the error is thrown in parse()\n",
        "    try:\n",
        "        article.parse()\n",
        "    except:\n",
        "        # add null values to show no connection\n",
        "        content_list.append(None)\n",
        "        date_list.append(None)\n",
        "        continue\n",
        "    content_list.append(article.text)\n",
        "    # this will be null if newspaper3k can't find it\n",
        "    date_list.append(article.publish_date)\n",
        "df['text'] = content_list\n",
        "df['date'] = date_list\n",
        "print('Number of entries with missing data:')\n",
        "print(df.isnull().sum(),'\\n')\n",
        "\n",
        "# drop any articles with missing data columns\n",
        "df = df.dropna()\n",
        "df = df.reset_index()\n",
        "df = df.drop(columns='index')\n",
        "print(f'Resulting entry count: {df.shape[0]}\\n')\n",
        "\n",
        "# convert date column to pandas Timestamps\n",
        "def timestampify(date):\n",
        "    return pd.Timestamp(date, unit='s').isoformat()\n",
        "df['date'] = df['date'].apply(timestampify)\n",
        "\n",
        "print('Filtering through police brutality filter...')\n",
        "# use NLP model to filter posts\n",
        "df['is_police_brutality'] = pipeline.predict(df['title'])\n",
        "df = df[df['is_police_brutality'] == 1]\n",
        "df = df.drop(columns='is_police_brutality')\n",
        "print(f'Number of entries determined to be about police brutality: {df.shape[0]}')\n",
        "\n",
        "# use spaCy to extract location tokens\n",
        "tokens_list = []\n",
        "print('Tokenizing through spaCy...')\n",
        "for text in df['text']:\n",
        "    doc = nlp(text)\n",
        "    ents = [e.text.lower() for e in doc.ents if e.label_ == 'GPE']\n",
        "    tokens_list.append(ents)\n",
        "df['tokens'] = tokens_list\n",
        "\n",
        "# figure out which city and state the article takes place in\n",
        "city_list = []\n",
        "state_list = []\n",
        "geo_list = []\n",
        "print('Compiling geolocation data...')\n",
        "for tokens in df['tokens']:\n",
        "    # set up Counter\n",
        "    c = Counter(tokens)\n",
        "\n",
        "    # set up geolocation dict for geo list\n",
        "    geo_entry = {'lat': None, 'long': None}\n",
        "\n",
        "    # count which states come back the most, if any\n",
        "    state_counts = {}\n",
        "    for state in states_map:\n",
        "        if c[state] > 0:\n",
        "            state_counts[state] = c[state]\n",
        "\n",
        "    # get state(s) that came back the most as dict with lists\n",
        "    max_count = 0\n",
        "    max_state = None\n",
        "\n",
        "    for state in state_counts:\n",
        "        if state_counts[state] > max_count:\n",
        "            max_count = state_counts[state]\n",
        "            max_state = {state: {}}\n",
        "        elif state_counts[state] == max_count:\n",
        "            max_state[state] = {}\n",
        "\n",
        "    # if no state is found\n",
        "    if max_state is None:\n",
        "        city_list.append(None)\n",
        "        state_list.append(None)\n",
        "        geo_list.append(geo_entry)\n",
        "        continue\n",
        "\n",
        "    max_city = None\n",
        "    # get any cities in tokens based on states\n",
        "    for state in max_state:  # ideally this should only run once\n",
        "        city_counts = {}\n",
        "        for city in states_map[state]:\n",
        "            if c[city] > 0:\n",
        "                city_counts[city] = c[city]\n",
        "        max_state[state] = city_counts\n",
        "\n",
        "        # get the city/state combo that came back the most\n",
        "        max_count = 0\n",
        "        for city in city_counts:\n",
        "            if city_counts[city] > max_count:\n",
        "                max_count = city_counts[city]\n",
        "                max_city = (city, state)\n",
        "\n",
        "    # if no city is found\n",
        "    if max_city is None:\n",
        "        city_list.append(None)\n",
        "        state_list.append(None)\n",
        "        geo_list.append(geo_entry)\n",
        "        continue\n",
        "\n",
        "    # the city and state should be known now\n",
        "\n",
        "    city_list.append(max_city[0].title())\n",
        "    state_list.append(max_city[1].title())\n",
        "    # now get the geolocation data\n",
        "    row = locs_df[(\n",
        "        (locs_df['city_ascii'] == max_city[0]) &\n",
        "        (locs_df['admin_name'] == max_city[1])\n",
        "    )]\n",
        "    row = row.reset_index()\n",
        "    if row.empty:\n",
        "        pass\n",
        "    else:\n",
        "        geo_entry['lat'] = row['lat'][0]\n",
        "        geo_entry['long'] = row['lng'][0]\n",
        "    geo_list.append(geo_entry)\n",
        "\n",
        "# loop ends, add cities and states onto dataframe\n",
        "df['city'] = city_list\n",
        "df['state'] = state_list\n",
        "df['geocoding'] = geo_list\n",
        "print('Number of entries where geolocation data could not be found:')\n",
        "print(df.isnull().sum(),'\\n')\n",
        "\n",
        "# drop any columns with null entries for location\n",
        "df = df.dropna()\n",
        "df = df.reset_index()\n",
        "df = df.drop(columns='index')\n",
        "\n",
        "# cleanup to match 846 api\n",
        "def listify(text):\n",
        "    return [text]\n",
        "df['links'] = df['url'].apply(listify)\n",
        "df['description'] = df['text']\n",
        "df = df.drop(columns=['tokens', 'text'])\n",
        "df = df[[\n",
        "    'id', 'state', 'city',\n",
        "    'date', 'title', 'description',\n",
        "    'links', 'geocoding'\n",
        "]]\n",
        "\n",
        "print(f'Final number of entries: {df.shape[0]}')\n",
        "df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pulling data from Reddit...\n",
            "Number of entries initially pulled: 500\n",
            "\n",
            "Extracting data via newspaper3k...\n",
            "Number of entries with missing data:\n",
            "id         0\n",
            "title      0\n",
            "url        0\n",
            "text      23\n",
            "date     175\n",
            "dtype: int64 \n",
            "\n",
            "Resulting entry count: 325\n",
            "\n",
            "Filtering through police brutality filter...\n",
            "Number of entries determined to be about police brutality: 18\n",
            "Tokenizing through spaCy...\n",
            "Compiling geolocation data...\n",
            "Number of entries where geolocation data could not be found:\n",
            "id            0\n",
            "title         0\n",
            "url           0\n",
            "text          0\n",
            "date          0\n",
            "tokens        0\n",
            "city         13\n",
            "state        13\n",
            "geocoding     0\n",
            "dtype: int64 \n",
            "\n",
            "Final number of entries: 18\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>state</th>\n",
              "      <th>city</th>\n",
              "      <th>date</th>\n",
              "      <th>title</th>\n",
              "      <th>description</th>\n",
              "      <th>links</th>\n",
              "      <th>geocoding</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>itf8aw</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2020-09-15T00:00:00</td>\n",
              "      <td>'Kushner Village' tenants sue to block paying ...</td>\n",
              "      <td>Washington (CNN) A group of tenants filed a la...</td>\n",
              "      <td>[https://edition.cnn.com/2020/09/15/politics/k...</td>\n",
              "      <td>{'lat': None, 'long': None}</td>\n",
              "      <td>[washington, new york city, kushner village, e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>itwaan</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2020-09-15T00:00:00</td>\n",
              "      <td>Documents Reveal How the Police Kept Daniel Pr...</td>\n",
              "      <td>ROCHESTER, N.Y. — It was early June, days afte...</td>\n",
              "      <td>[https://www.nytimes.com/2020/09/15/nyregion/r...</td>\n",
              "      <td>{'lat': None, 'long': None}</td>\n",
              "      <td>[rochester, n.y., rochester, rochester]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>iqixfl</td>\n",
              "      <td>California</td>\n",
              "      <td>San Francisco</td>\n",
              "      <td>2020-09-11T02:41:24+00:00</td>\n",
              "      <td>Tiny California Town Leveled By “Massive Wall ...</td>\n",
              "      <td>In a scene that brought back nightmares of the...</td>\n",
              "      <td>[https://deadline.com/2020/09/california-town-...</td>\n",
              "      <td>{'lat': 37.7562, 'long': -122.443}</td>\n",
              "      <td>[oroville, concow, san francisco, berry creek,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>isy5uf</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2020-09-15T00:00:00</td>\n",
              "      <td>Police officer suspended over Melbourne head-s...</td>\n",
              "      <td>Victoria's anti-corruption watchdog will take ...</td>\n",
              "      <td>[https://www.abc.net.au/news/2020-09-15/police...</td>\n",
              "      <td>{'lat': None, 'long': None}</td>\n",
              "      <td>[victoria, melbourne]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>itff55</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2020-09-15T09:22:00-04:00</td>\n",
              "      <td>U.S. drops tariffs on Canadian aluminum, avoid...</td>\n",
              "      <td>OTTAWA -- The federal government is celebratin...</td>\n",
              "      <td>[https://www.ctvnews.ca/politics/u-s-drops-tar...</td>\n",
              "      <td>{'lat': None, 'long': None}</td>\n",
              "      <td>[the united states’, u.s., u.s., lighthizer, c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  ...                                             tokens\n",
              "5   itf8aw  ...  [washington, new york city, kushner village, e...\n",
              "45  itwaan  ...            [rochester, n.y., rochester, rochester]\n",
              "48  iqixfl  ...  [oroville, concow, san francisco, berry creek,...\n",
              "76  isy5uf  ...                              [victoria, melbourne]\n",
              "95  itff55  ...  [the united states’, u.s., u.s., lighthizer, c...\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3F3gsBt9FYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}